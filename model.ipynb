{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, features, labels=None):\n",
    "#         self.features = features\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.labels is not None:\n",
    "#             return self.features[idx], self.labels[idx]\n",
    "#         return self.features[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from pymongo import MongoClient\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Define your ANN model\n",
    "# class ANN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(ANN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Handling both single samples and batches\n",
    "#         if len(x.shape) == 1:\n",
    "#             x = x.unsqueeze(0)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x.squeeze()\n",
    "\n",
    "\n",
    "# # Define your dataset\n",
    "# # Define your dataset\n",
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, cursor):\n",
    "#         self.cursor = cursor\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.features = []\n",
    "\n",
    "#         # Iterate through documents and extract features\n",
    "#         for doc in self.cursor:\n",
    "#             if 'features' in doc and 'mfccs_stats' in doc['features']:\n",
    "#                 mfccs_mean = np.array(doc['features']['mfccs_stats']['mean'])\n",
    "\n",
    "#                 # Reshape mfccs_mean to have a fixed size (1, 20)\n",
    "#                 mfccs_mean = mfccs_mean[:20]  # Trim or pad as necessary\n",
    "#                 mfccs_mean = np.pad(mfccs_mean, (0, 20 - len(mfccs_mean)), mode='constant')  # Pad if less than 20\n",
    "#                 song_features = mfccs_mean\n",
    "\n",
    "#                 self.features.append(song_features)\n",
    "\n",
    "#         # Standardize features\n",
    "#         self.features = np.array(self.features)\n",
    "#         print(\"Shape of self.features before scaling:\", self.features.shape)\n",
    "\n",
    "#         self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'features': torch.from_numpy(self.features[idx]).float().unsqueeze(0)  # Reshape to (1, 20)\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Setup MongoDB connection\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['Dataset']\n",
    "# collection = db['audio']\n",
    "\n",
    "# # Fetch documents\n",
    "# documents = collection.find({}).limit(119415)  # Limiting to accomodate data\n",
    "\n",
    "# # Initialize Dataset and DataLoader\n",
    "# dataset = AudioDataset(documents)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Define model, optimizer, and loss function\n",
    "# model = ANN(input_size=dataset.features.shape[1], hidden_size=64)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(100):\n",
    "#     for batch in dataloader:\n",
    "#         features = batch['features']\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(features)\n",
    "#         loss = criterion(outputs, torch.zeros_like(outputs))  # Modify target according to your task\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "# # Now you can use the trained model for music recommendation\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define your training parameters\n",
    "# num_epochs = 100\n",
    "# kf = KFold(n_splits=5, shuffle=True)  # 5-fold cross-validation\n",
    "\n",
    "# # List to store validation losses for each fold\n",
    "# validation_losses = []\n",
    "\n",
    "# # Perform cross-validation\n",
    "# for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
    "#     print(f\"Fold {fold + 1}:\")\n",
    "\n",
    "#     # Split the dataset into training and validation sets\n",
    "#     train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "#     val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "#     # Initialize model, optimizer, and loss function\n",
    "#     model = ANN(input_size=dataset.features.shape[1], hidden_size=64)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = nn.MSELoss()\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for batch in train_loader:\n",
    "#             features = batch['features']\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(features)\n",
    "#             loss = criterion(outputs, torch.zeros_like(outputs))  # Modify target according to your task\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     # Evaluate the model on the validation set\n",
    "#     val_losses = []\n",
    "#     for batch in val_loader:\n",
    "#         features = batch['features']\n",
    "#         outputs = model(features)\n",
    "#         loss = criterion(outputs, torch.zeros_like(outputs))  # Modify target according to your task\n",
    "#         val_losses.append(loss.item())\n",
    "    \n",
    "#     # Calculate average validation loss for this fold\n",
    "#     avg_val_loss = np.mean(val_losses)\n",
    "#     validation_losses.append(avg_val_loss)\n",
    "#     print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# # Calculate average validation loss across all folds\n",
    "# avg_validation_loss = np.mean(validation_losses)\n",
    "# print(f\"Average Validation Loss: {avg_validation_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Training loop\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     print(f\"Epoch {epoch + 1}:\")\n",
    "    #     for batch_idx, batch in enumerate(train_loader):\n",
    "    #         features = batch['features']\n",
    "            \n",
    "    #         optimizer.zero_grad()\n",
    "    #         outputs = model(features)\n",
    "    #         loss = criterion(outputs, torch.zeros_like(outputs))  # Modify target according to your task\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "            \n",
    "    #         print(f\"\\tBatch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import numpy as np\n",
    "# from pymongo import MongoClient\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Correct the class definition\n",
    "# class ANN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):  # Correct use of double underscores\n",
    "#         super(ANN, self).__init__()  # Correct use of double underscores for superclass initializer\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "       \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Function to load the model\n",
    "# def load_model(model_path, input_size, hidden_size):\n",
    "#     model = ANN(input_size, hidden_size)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Example usage\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['Dataset']\n",
    "# collection = db['audio']\n",
    "# model = load_model('model.pth', input_size=20, hidden_size=64)\n",
    "\n",
    "# # Assuming the rest of your function to fetch and process data is correct,\n",
    "# # your model is now correctly defined and can be used as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from pymongo import MongoClient\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "\n",
    "# def fetch_features(db, song_id=None, filename=None):\n",
    "#     \"\"\" Fetch features for a specific song by ID or filename from MongoDB. \"\"\"\n",
    "#     query = {\"_id\": song_id} if song_id else {\"filename\": filename}\n",
    "#     song_data = db.audio.find_one(query)\n",
    "#     if not song_data:\n",
    "#         return None\n",
    "\n",
    "#     mfccs = np.array(song_data['features']['mfccs_stats']['mean'])\n",
    "#     features = np.reshape(mfccs, (1, -1))  # Reshape for single sample\n",
    "#     return features\n",
    "\n",
    "# def find_similar_songs(model, db, input_features, num_results=5):\n",
    "#     \"\"\" Find songs similar to the provided features using the trained model. \"\"\"\n",
    "#     scaler = StandardScaler()\n",
    "\n",
    "#     # Process input features\n",
    "#     input_features = scaler.fit_transform(input_features)\n",
    "#     input_vector = torch.tensor(input_features, dtype=torch.float32)\n",
    "#     input_vector = model(input_vector).detach().numpy()\n",
    "\n",
    "#     # Fetch all songs features\n",
    "#     all_songs = db.audio.find()\n",
    "#     all_features = []\n",
    "#     song_ids = []\n",
    "\n",
    "#     for song in all_songs:\n",
    "#         if 'features' in song and 'mfccs_stats' in song['features']:\n",
    "#             mfccs = np.array(song['features']['mfccs_stats']['mean'])\n",
    "#             all_features.append(mfccs)\n",
    "#             song_ids.append(song['_id'])\n",
    "#         else:\n",
    "#             print(f\"Skipping song {song['_id']} because it doesn't have mfccs_stats\")\n",
    "\n",
    "#     all_features = np.array(all_features)\n",
    "\n",
    "#     all_features = scaler.transform(all_features)\n",
    "#     all_features_tensor = torch.tensor(all_features, dtype=torch.float32)\n",
    "#     all_vectors = model(all_features_tensor).detach().numpy()\n",
    "\n",
    "#     # Calculate distances and find the most similar songs\n",
    "#     distances = np.linalg.norm(all_vectors - input_vector, axis=1)\n",
    "#     nearest_indices = np.argsort(distances)[:num_results]\n",
    "#     return [song_ids[i] for i in nearest_indices]\n",
    "\n",
    "# # Example usage:\n",
    "# model = load_model('model.pth', input_size=20, hidden_size=64)  # Use the correct input_size and hidden_size\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['Dataset']\n",
    "\n",
    "# # Fetch features for a specific song by ID or filename\n",
    "# features = fetch_features(db, filename=\"000405.mp3\")\n",
    "\n",
    "# if features is not None:\n",
    "#     similar_songs = find_similar_songs(model, db, features)\n",
    "#     print(\"Similar songs IDs:\", similar_songs)\n",
    "# else:\n",
    "#     print(\"Song not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from bson import ObjectId\n",
    "\n",
    "# # Load the model, connect to MongoDB, and define functions fetch_features and find_similar_songs\n",
    "\n",
    "# model = load_model('model.pth', input_size=20, hidden_size=64)  # Load the model\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "# db = client['Dataset']\n",
    "\n",
    "# similar_songs_data = {}  # Dictionary to store input data and related song data\n",
    "\n",
    "# # Fetch the first 200 rows from MongoDB\n",
    "# cursor = db.audio.find().skip(90).limit(30)\n",
    "\n",
    "# for row in cursor:\n",
    "#     # Fetch features for the song\n",
    "#     features = fetch_features(db, song_id=row['_id'])\n",
    "    \n",
    "#     if features is not None:\n",
    "#         # Find similar songs\n",
    "#         similar_songs_ids = find_similar_songs(model, db, features)\n",
    "        \n",
    "#         # Store input data (song data) and related song data in the dictionary\n",
    "#         input_data = {\n",
    "#             'input_data': {\n",
    "#                 '_id': str(row['_id']),\n",
    "#                 'filename': row['filename'],  # Assuming 'filename' is present in the document\n",
    "#                 'metadata': row.get('metadata', {})  # Assuming 'metadata' is present in the document\n",
    "#             },\n",
    "#             'similar_songs': {}\n",
    "#         }\n",
    "        \n",
    "#         for song_id in similar_songs_ids:\n",
    "#             similar_song_data = db.audio.find_one({'_id': ObjectId(song_id)})\n",
    "#             if similar_song_data:\n",
    "#                 similar_song = {\n",
    "#                     '_id': str(similar_song_data['_id']),\n",
    "#                     'filename': similar_song_data['filename'],  # Assuming 'filename' is present in the document\n",
    "#                     'metadata': similar_song_data.get('metadata', {})  # Assuming 'metadata' is present in the document\n",
    "#                 }\n",
    "#                 input_data['similar_songs'][str(song_id)] = similar_song\n",
    "        \n",
    "#         similar_songs_data[str(row['_id'])] = input_data\n",
    "\n",
    "# # Write the collected data to a JSON file\n",
    "# with open('similar_songs_data.json', 'w') as json_file:\n",
    "#     json.dump(similar_songs_data, json_file, default=str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
